{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import humanize\n",
    "import psutil\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import networkx as nx\n",
    "import igraph\n",
    "import leidenalg\n",
    "\n",
    "import numpy as np\n",
    "from wisdom_of_crowds import Crowd\n",
    "from wisdom_of_crowds import make_sullivanplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, MiniBatchNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_STRING = \"mongodb://JamIs:morticiaetpollito@118.138.244.29:27017/\"\n",
    "\n",
    "class Tweet:\n",
    "\tdef __init__(self, tweet):\n",
    "\t\tself.id = tweet['id']\n",
    "\t\tself.user = tweet['user']\n",
    "\t\tself.connected_user = tweet['connected_user']\n",
    "\t\tself.connection_type = tweet['connection_type']\n",
    "\t\tself.text = tweet['text']\n",
    "\n",
    "query = {\"$and\":\n",
    "\t\t\t[\n",
    "\t\t\t\t{\"datetime\": {\"$eq\": None}},\n",
    "\t\t\t\t{\"lang\": 'en'},\n",
    "\t\t\t\t{\"connection_type\": {\"$exists\": \"true\"}},\n",
    "\t\t\t\t{\"connection_type\": {\"$ne\": None}},\n",
    "\t\t\t\t{\"connected_user\": {\"$ne\": None}}\n",
    "\t\t\t]\n",
    "\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting...\n",
      "connected\n"
     ]
    }
   ],
   "source": [
    "print(\"connecting...\")\n",
    "client = MongoClient(CONNECTION_STRING)\n",
    "tw_coll = client.get_database('Tw_Covid_DB').get_collection('tweets')\n",
    "tu_coll = client.get_database('Tw_Covid_DB').get_collection('users')\n",
    "print(\"connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 182020\n"
     ]
    }
   ],
   "source": [
    "dates = {}\n",
    "\n",
    "for tweet in tw_coll.find():\n",
    "\t#check date of tweet\n",
    "\tmessy_date = tweet[\"created_at\"] #\"Thu Mar 12 02:01:57 +0000 2020\"\n",
    "\treal_date = messy_date[:10] + messy_date[-4:]\n",
    "\n",
    "\tif real_date in dates.keys():\n",
    "\t\t#increment counter for each date\n",
    "\t\tdates[real_date] += 1\n",
    "\telse:\n",
    "\t\tdates[real_date] = 1\n",
    "\n",
    "best_date = None\n",
    "for date in dates.keys():\n",
    "\tif best_date is None:\n",
    "\t\tbest_date = date\n",
    "\tif dates[best_date] < dates[date]:\n",
    "\t\tbest_date = date\n",
    "\n",
    "print(best_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7616588\n"
     ]
    }
   ],
   "source": [
    "query_results = tw_coll.find(query)\n",
    "db_tweets = []\n",
    "for t in query_results:\n",
    "\tdb_tweets.append(Tweet(t))\n",
    "print(len(db_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dict' object has no attribute 'user'\n",
      "Tweets loaded: 0\n",
      "Time taken: 0 seconds\n",
      "Memory used: 11.2 GB\n"
     ]
    }
   ],
   "source": [
    "tweets_by_id = {}\n",
    "final_tweets = {}\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "\tfor t in tweets:\n",
    "\t\t#we do a mapping of tweets to users this way so we can filter users based on ...location? eventually\n",
    "\t\t#user_tweet_mapping is an interim mapping we use to get the users from the database. At query time we add any additional criteria\n",
    "\t\t#tweets is a interm list of Tweets objects so we can keep the tweet id, users id, and connected_user id bundled during the batched query\n",
    "\t\ttweets_by_id[t.id] = t\n",
    "\t\tif t.user not in user_tweet_mapping:\n",
    "\t\t\tuser_tweet_mapping[t.user] = []\n",
    "\t\tif t.connected_user not in user_tweet_mapping:\n",
    "\t\t\tuser_tweet_mapping[t.connected_user] = []\n",
    "\t\tuser_tweet_mapping[t.user].append(t.id) \n",
    "\t\tuser_tweet_mapping[t.connected_user].append(t.id)\n",
    "\t\t\n",
    "\t\tif len(user_tweet_mapping) > 4096:\n",
    "\t\t\tuser_list = user_tweet_mapping.keys()\n",
    "\t\t\tusers = tu_coll.find({\"_id\":{\"$in\":list(user_list)}})#list()}}) #get the users\n",
    "\t\t\tfor user in users:\n",
    "\t\t\t\t# if the user is found by our constrained query, the Tweet object with the bundled tweet id, users id, and connected_user id\n",
    "\t\t\t\t# is added to the final_tweets list. The final_tweets list therefore only has tweets by users who meet our criteria\n",
    "\t\t\t\tfor tweet_id in user_tweet_mapping[user['id']]:\n",
    "\t\t\t\t\tfinal_tweets[tweet_id] = tweets[tweet_id]\n",
    "\t\t\tuser_tweet_mapping = {} #reset\n",
    "except Exception as e:\n",
    "\tprint(e)\n",
    "finally:\n",
    "\tend_time = time.time()\n",
    "\tprint(\"Tweets loaded: {}\".format(len(final_tweets)))\n",
    "\tprint(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "\tprint(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ig_plot(i_g, layout=\"auto\", filename=None):\n",
    "\tprint(f\"plotting {layout}:\")\n",
    "\tfilename = filename or f\"ig_{layout}.pdf\"\n",
    "\tstart_time = time.time()\n",
    "\tif hasattr(i_g, 'membership'):\n",
    "\t\tvertex_colour = i_g.membership\n",
    "\telse:\n",
    "\t\tvertex_colour = \"black\"\n",
    "\tigraph.plot(\n",
    "\t\ti_g, \n",
    "\t\tlayout=layout, \n",
    "\t\ttarget=filename, \n",
    "\t\tvertex_size=5, \n",
    "\t\tedge_arrow_size=0.5, \n",
    "\t\tedge_arrow_width = 0.5,\n",
    "\t\t#vertex_label = i_g.vs[\"name\"],\n",
    "\t\tvertex_color=vertex_colour,\n",
    "\t\tpalette=igraph.RainbowPalette(),\n",
    "\t)\n",
    "\tend_time = time.time()\n",
    "\tprint(\"plotting complete\")\n",
    "\tprint(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "\tprint(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building networkx graph...\n",
      "Time taken: 3 minutes and 59 seconds\n",
      "Memory used: 10.7 GB\n",
      "Nodes: 3617873\n",
      "Edges: 7220344\n",
      "Density: 5.516344421444012e-07\n",
      "networkx graph built\n"
     ]
    }
   ],
   "source": [
    "nx_g = nx.DiGraph()\n",
    "print(\"building networkx graph...\")\n",
    "start_time = time.time()\n",
    "for tweet in db_tweets:\n",
    "\tif tweet.user not in nx_g:\n",
    "\t\tnx_g.add_node(tweet.user)\n",
    "\t\n",
    "\tif tweet.connected_user not in nx_g:\n",
    "\t\tnx_g.add_node(tweet.connected_user)\n",
    "\t\n",
    "\tif tweet.connected_user != tweet.user:\n",
    "\t\tif tweet.connected_user not in nx_g[tweet.user]:\n",
    "\t\t\tnx_g.add_edge(tweet.user, tweet.connected_user, weight=1, tweets=[tweet.id])\n",
    "\t\telse:\n",
    "\t\t\tnx_g[tweet.user][tweet.connected_user]['weight'] += 1\n",
    "\t\t\tnx_g[tweet.user][tweet.connected_user]['tweets'] += tweet.id\n",
    "end_time = time.time()\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "print(\"Nodes: {}\".format(nx_g.number_of_nodes()))\n",
    "print(\"Edges: {}\".format(nx_g.number_of_edges()))\n",
    "print(\"Density: {}\".format(nx.density(nx_g)))\n",
    "print(\"networkx graph built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how to find clusters on networkx graph\n",
    "https://stackoverflow.com/questions/43541376/how-to-find-clusters-in-networkx-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building igraph from networkx graph\n",
      "Time taken: 40 seconds\n",
      "Memory used: 9.2 GB\n",
      "Nodes: 3617873\n",
      "Edges: 7220344\n",
      "Transitivity: 0.00010257998573380519\n",
      "igraph from networkx built\n"
     ]
    }
   ],
   "source": [
    "print(\"building igraph from networkx graph\")\n",
    "start_time = time.time()\n",
    "i_g = igraph.Graph.from_networkx(nx_g, vertex_attr_hashable=\"name\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "print(\"Nodes: {}\".format(len(i_g.vs)))\n",
    "print(\"Edges: {}\".format(len(i_g.es)))\n",
    "print(\"Transitivity: {}\".format(i_g.transitivity_undirected()))\n",
    "print(\"igraph from networkx built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leidenalg:\n",
      "Graphs: 1966\n",
      "leidenalg analysis complete\n",
      "Time taken: 49 seconds\n",
      "Memory used: 8.2 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"leidenalg:\")\n",
    "start_time = time.time()\n",
    "ig_community_graph = leidenalg.find_partition(i_g.connected_components(\"weak\").giant(), leidenalg.ModularityVertexPartition);\n",
    "print(\"Graphs: {}\".format(len(ig_community_graph.subgraphs())))\n",
    "end_time = time.time()\n",
    "print(\"leidenalg analysis complete\")\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting fruchterman_reingold:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mig_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mig_community_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfruchterman_reingold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleidencommunities.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36mig_plot\u001b[1;34m(i_g, layout, filename)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m \tvertex_colour \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43migraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mi_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mlayout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mvertex_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m\t\u001b[49m\u001b[43medge_arrow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m\t\u001b[49m\u001b[43medge_arrow_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;66;43;03m#vertex_label = i_g.vs[\"name\"],\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mvertex_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_colour\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mpalette\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43migraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRainbowPalette\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplotting complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\drawing\\__init__.py:319\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(obj, target, bbox, *args, **kwds)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# We are either not in IPython or the user specified an explicit plot target,\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# so just show or save the result\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# save\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Also return the plot itself\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\drawing\\cairo\\plot.py:306\u001b[0m, in \u001b[0;36mCairoPlot.save\u001b[1;34m(self, fname)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;124;03m\"\"\"Saves the plot.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m@param fname: the filename to save to. It is ignored if the surface\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m  of the plot is not an C{ImageSurface}.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_dirty:\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_surface, cairo\u001b[38;5;241m.\u001b[39mImageSurface):\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_tmpfile:\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\drawing\\cairo\\plot.py:283\u001b[0m, in \u001b[0;36mCairoPlot.redraw\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m--> 283\u001b[0m plotter(\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcairo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    285\u001b[0m     ctx,\n\u001b[0;32m    286\u001b[0m     bbox\u001b[38;5;241m=\u001b[39mbbox,\n\u001b[0;32m    287\u001b[0m     palette\u001b[38;5;241m=\u001b[39mpalette,\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;241m*\u001b[39margs,  \u001b[38;5;66;03m# noqa: B026\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[0;32m    290\u001b[0m )\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opacity \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m    292\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mpop_group_to_source()\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\clustering.py:507\u001b[0m, in \u001b[0;36mVertexClustering.__plot__\u001b[1;34m(self, backend, context, *args, **kwds)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_color\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[0;32m    506\u001b[0m     kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_color\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmembership\n\u001b[1;32m--> 507\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39m__plot__(backend, context, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\drawing\\graph.py:561\u001b[0m, in \u001b[0;36m__plot__\u001b[1;34m(self, backend, context, *args, **kwds)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01migraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrawing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DrawerDirectory\n\u001b[0;32m    557\u001b[0m drawer \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrawer_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    559\u001b[0m     DrawerDirectory\u001b[38;5;241m.\u001b[39mresolve(\u001b[38;5;28mself\u001b[39m, backend)(context),\n\u001b[0;32m    560\u001b[0m )\n\u001b[1;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m drawer\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\drawing\\cairo\\graph.py:141\u001b[0m, in \u001b[0;36mCairoGraphDrawer.draw\u001b[1;34m(self, graph, *args, **kwds)\u001b[0m\n\u001b[0;32m    138\u001b[0m palette \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpalette\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Calculate/get the layout of the graph\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m layout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Determine the size of the margin on each side\u001b[39;00m\n\u001b[0;32m    144\u001b[0m margin \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmargin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\drawing\\baseclasses.py:297\u001b[0m, in \u001b[0;36mAbstractGraphDrawer.ensure_layout\u001b[1;34m(layout, graph)\u001b[0m\n\u001b[0;32m    295\u001b[0m     layout \u001b[38;5;241m=\u001b[39m Layout(layout\u001b[38;5;241m.\u001b[39mcoords)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layout, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 297\u001b[0m     layout \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    299\u001b[0m     (layout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mattributes())\n\u001b[0;32m    302\u001b[0m ):\n\u001b[0;32m    303\u001b[0m     layout \u001b[38;5;241m=\u001b[39m AbstractGraphDrawer\u001b[38;5;241m.\u001b[39mensure_layout(graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout\u001b[39m\u001b[38;5;124m\"\u001b[39m], graph\u001b[38;5;241m=\u001b[39mgraph)\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\layout.py:531\u001b[0m, in \u001b[0;36m_layout\u001b[1;34m(graph, layout, *args, **kwds)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(method):\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout method must be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 531\u001b[0m layout \u001b[38;5;241m=\u001b[39m method(graph, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layout, Layout):\n\u001b[0;32m    533\u001b[0m     layout \u001b[38;5;241m=\u001b[39m Layout(layout)\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\layout.py:693\u001b[0m, in \u001b[0;36m_layout_method_wrapper.<locals>.result\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    691\u001b[0m layout \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layout, Layout):\n\u001b[1;32m--> 693\u001b[0m     layout \u001b[38;5;241m=\u001b[39m \u001b[43mLayout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layout\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\layout.py:73\u001b[0m, in \u001b[0;36mLayout.__init__\u001b[1;34m(self, coords, dim)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"Constructor.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m@param coords: the coordinates to be stored in the layout.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03mis.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(coord) \u001b[38;5;28;01mfor\u001b[39;00m coord \u001b[38;5;129;01min\u001b[39;00m coords]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coords \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\igraph\\layout.py:73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"Constructor.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m@param coords: the coordinates to be stored in the layout.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03mis.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(coord) \u001b[38;5;28;01mfor\u001b[39;00m coord \u001b[38;5;129;01min\u001b[39;00m coords]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coords \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ig_plot(ig_community_graph, \"fruchterman_reingold\", filename=\"leidencommunities.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 0: 635463 nodes\n",
      "Community 0 as a proportion of total: 0.17564546903664113\n",
      "Community 1: 386816 nodes\n",
      "Community 1 as a proportion of total: 0.10691807036897094\n",
      "Community 2: 222038 nodes\n",
      "Community 2 as a proportion of total: 0.0613725246850843\n",
      "Community 3: 220083 nodes\n",
      "Community 3 as a proportion of total: 0.060832151930153434\n",
      "Small Graphs: 1551\n"
     ]
    }
   ],
   "source": [
    "# Get all subgraphs\n",
    "subgraphs = ig_community_graph.subgraphs()\n",
    "# Sort subgraphs by size in descending order\n",
    "sorted_subgraphs = sorted(subgraphs, key=lambda x: len(x.vs), reverse=True)\n",
    "# Get the largest 4 subgraphs\n",
    "largest_4_subgraphs = sorted_subgraphs[:4]\n",
    "for i, subgraph in enumerate(largest_4_subgraphs):\n",
    "\tprint(f\"Community {i}: {len(subgraph.vs)} nodes\")\n",
    "\tprint(f\"Community {i} as a proportion of total: {len(subgraph.vs)/len(i_g.vs)}\")\n",
    "# Filter subgraphs with less than 10 nodes\n",
    "small_subgraphs = [sg for sg in subgraphs if len(sg.vs) < 10]\n",
    "print(\"Small Graphs: {}\".format(len(small_subgraphs)))\n",
    "# Count the number of small subgraphs\n",
    "num_small_subgraphs = len(small_subgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five edges in community 0:\n",
      "4851 -> 15094: 5 tweets\n",
      "76692 -> 15094: 4 tweets\n",
      "100057 -> 100058: 4 tweets\n",
      "790 -> 8676: 3 tweets\n",
      "2351 -> 32845: 3 tweets\n",
      "Top five edges in community 1:\n",
      "208760 -> 711: 52 tweets\n",
      "94245 -> 173: 48 tweets\n",
      "11298 -> 11297: 44 tweets\n",
      "9914 -> 72: 43 tweets\n",
      "4097 -> 72: 42 tweets\n",
      "Top five edges in community 2:\n",
      "538 -> 15: 95 tweets\n",
      "32879 -> 7492: 62 tweets\n",
      "20532 -> 13677: 47 tweets\n",
      "130122 -> 13677: 44 tweets\n",
      "27917 -> 1889: 40 tweets\n",
      "Top five edges in community 3:\n",
      "3411 -> 3412: 72 tweets\n",
      "9128 -> 3412: 62 tweets\n",
      "10417 -> 10418: 52 tweets\n",
      "217418 -> 216609: 38 tweets\n",
      "217503 -> 216609: 38 tweets\n"
     ]
    }
   ],
   "source": [
    "for i, subgraph in enumerate(largest_4_subgraphs):\n",
    "\t#order the edges in a community by weight\n",
    "\tedges = sorted(subgraph.es, key=lambda edge: edge['weight'], reverse=True)\n",
    "\ttop_five = edges[:5]\n",
    "\n",
    "\t# Print the top five edges in the community\n",
    "\tprint(f\"Top five edges in community {i}:\")\n",
    "\tfor edge in top_five:\n",
    "\t\tprint(f\"{edge.source} -> {edge.target}: {edge['weight']} tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community  0\n",
      "Node id:  470021270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1134, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 311, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"c:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2062, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"c:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2098, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m central_nodes:\n\u001b[0;32m     14\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode id: \u001b[39m\u001b[38;5;124m\"\u001b[39m, node[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 15\u001b[0m \t\u001b[38;5;28;43mprint\u001b[39;49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode degree: \u001b[39m\u001b[38;5;124m\"\u001b[39m, node\u001b[38;5;241m.\u001b[39mdegree())\n\u001b[0;32m     17\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m db_tweets:\n\u001b[0;32m     18\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m tweet\u001b[38;5;241m.\u001b[39muser \u001b[38;5;241m==\u001b[39m node[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m central_nodes:\n\u001b[0;32m     14\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode id: \u001b[39m\u001b[38;5;124m\"\u001b[39m, node[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 15\u001b[0m \t\u001b[38;5;28;43mprint\u001b[39;49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode degree: \u001b[39m\u001b[38;5;124m\"\u001b[39m, node\u001b[38;5;241m.\u001b[39mdegree())\n\u001b[0;32m     17\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m db_tweets:\n\u001b[0;32m     18\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m tweet\u001b[38;5;241m.\u001b[39muser \u001b[38;5;241m==\u001b[39m node[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1443\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:700\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1143\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1134\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2062\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2059\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2061\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[1;32m-> 2062\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2064\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2067\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Isis Urgell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2098\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2095\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2098\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Task: Inspecting the speech of the top five nodes in each community to label community type\n",
    "\n",
    "#1 Get higher centrality nodes in each subgraph\n",
    "for i, community in enumerate(largest_4_subgraphs):\n",
    "\tprint (\"Community \", i)\n",
    "\t#Find higher centrality nodes in each subgraph\n",
    "\tnodes = sorted(community.vs, key=lambda vertex: vertex.degree(), reverse=True)\n",
    "\tcentral_nodes = nodes[:5]\n",
    "\t\n",
    "\n",
    "    #2 Get tweets from higher centrality nodes in each subgraph\n",
    "\t#get each edge for each author\n",
    "\tfor node in central_nodes:\n",
    "\t\tprint(\"Node id: \", node['name'])\n",
    "\t\tprint(\"Node degree: \", node.degree())\n",
    "\n",
    "\t\tfor tweet in db_tweets:\n",
    "\t\t\tif tweet.user == node['name']:\n",
    "\t\t\t\tprint(tweet.text)\n",
    "\t\t\n",
    "\t\t#get each tweet on each edge\n",
    "\t\t# for edge in edges:\n",
    "\t\t# \ttweets = edge['tweets']\n",
    "\t\t# \tfor tweet_id in tweets:\n",
    "\t\t# \t\ttweet = final_tweets[tweet_id]\n",
    "\t\t# \t\tprint(tweet['text'])  # Fix: Access the 'text' attribute of the 'tweet' object instead of the 'tweet_id' variable.\n",
    "\n",
    "#3 Print the text of the Tweet ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transferring igraph communities to networkx\n",
      "0\n",
      "3617873\n",
      "Time taken: 57 seconds\n",
      "Memory used: 12.5 GB\n",
      "igraph communities transferred to networkx\n"
     ]
    }
   ],
   "source": [
    "print(\"transferring igraph communities to networkx\")\n",
    "start_time = time.time()\n",
    "#shift community info from igraph to networkx\n",
    "#for each node in networkx graph\n",
    "#look up matching node in igraph\n",
    "#assign T property of networkx graph node to community value from igraph node\n",
    "# Get the membership list from the igraph partition\n",
    "membership = ig_community_graph.membership\n",
    "# For each node in the networkx graph\n",
    "for node in nx_g.nodes():\n",
    "\t# Look up the matching node in the igraph graph\n",
    "\tig_node_index = i_g.vs.find('_nx_name'==node).index\n",
    "\t# Assign the 'T' property of the networkx node to the community value from the igraph node\n",
    "\tnx_g.nodes[node]['T'] = membership[ig_node_index]\n",
    "#delete all nodes that don't have a membership value - these nodes weren't in the igraph largest connected component\n",
    "print(len([node for node in nx_g if 'T' not in nx_g.nodes[node]]))\n",
    "print(len([node for node in nx_g if 'T' in nx_g.nodes[node]]))\n",
    "nx_g.remove_nodes_from([node for node in nx_g if 'T' not in nx_g.nodes[node]])\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "print(\"igraph communities transferred to networkx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do wisdom of the crowds analysis\n",
    "print(\"wisdom of the crowds:\")\n",
    "start_time = time.time()\n",
    "c = Crowd(nx_g)\n",
    "s_set = []\n",
    "d_set = []\n",
    "for node in c.node_set:\n",
    "\ts_set.append(c.S(node))\n",
    "\td_set.append(c.D(node))\n",
    "s_set = np.array(s_set)\n",
    "d_set = np.array(d_set)\n",
    "π_set = np.multiply(s_set,d_set)\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "print(\"wisdom of the crowds complete\")\n",
    "make_sullivanplot(π_set,d_set,s_set)\n",
    "make_sullivanplot(π_set,d_set,s_set,colormap='magma_r',yscale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "init = \"nndsvda\"\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorising (TF-IDF)...\n",
      "n_samples/documents: 7616588, n_features/words: 9\n",
      "Sparsity (number of cells with non-zero values): 0.219\n",
      "vectorization done in 1 minute and 44 seconds\n",
      "Memory used: 12.8 GB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Add your custom words\n",
    "# Note: “WuhanVirus” not included as it is politically charged\n",
    "custom_words = [\"coronavirus\", \"2019nCoV\", \"corona virus\", \"COVD19\", \"CoronavirusPandemic\", \"COVID-19\", \"CoronaOutbreak\", \n",
    "\t\t\t\t\"pneumonia\", \"pneumonie\", \"neumonia\", \"lungenentzündung\", \"COVID19\", \"http\", \"https\", \"https://\", \"19\", \"just\", \"people\", \"rt\"]\n",
    "\n",
    "# Extend the default English stop words list with your words\n",
    "stop_words = ENGLISH_STOP_WORDS.union(custom_words)\n",
    "\n",
    "print(\"Vectorising (TF-IDF)...\")\n",
    "TFIDFvectorizer = TfidfVectorizer(\n",
    "\tmin_df=0.05,\n",
    "\tstop_words=stop_words\n",
    ")\n",
    "start_time = time.time()\n",
    "TFIDFvectorised_dataset = TFIDFvectorizer.fit_transform([tweet.text for tweet in tweets])\n",
    "print(f\"n_samples/documents: {TFIDFvectorised_dataset.shape[0]}, n_features/words: {TFIDFvectorised_dataset.shape[1]}\") #shape is rows of the matrix in that dimension\n",
    "print(f\"Sparsity (number of cells with non-zero values): {TFIDFvectorised_dataset.nnz / np.prod(TFIDFvectorised_dataset.shape):.3f}\")\n",
    "print(f\"vectorization done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering...\n",
      "Number of elements assigned to each cluster: [ 624549 1629895 1929417  490726  699247  812991  497375  446619  148491\n",
      "  337278]\n",
      "Number of elements assigned to each cluster: [ 105428 1929417  342789  476218 1308763  624745  497380  435971  366553\n",
      " 1529324]\n",
      "Number of elements assigned to each cluster: [1629895 1929417  699247  490726  497375  446619  624549  337278  812991\n",
      "  148491]\n",
      "Number of elements assigned to each cluster: [ 435774 1629921 1929417 1304531  342789  497375  624549  366267  337278\n",
      "  148687]\n",
      "Number of elements assigned to each cluster: [1313588 1929417 1629921   54266  342789  476214  497380  365438  435972\n",
      "  571603]\n"
     ]
    }
   ],
   "source": [
    "print(\"Kmeans clustering...\")\n",
    "\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
    "\tname = km.__class__.__name__ if name is None else name\n",
    "\n",
    "\ttrain_times = []\n",
    "\tscores = [] #score = \"Silhouette Coefficient\"\n",
    "\tfor seed in range(n_runs):\n",
    "\t\tkm.set_params(random_state=seed)\n",
    "\t\tstart_time = time.time()\n",
    "\t\tkm.fit(X)\n",
    "\t\ttrain_times.append(time.time() - start_time)\n",
    "\t\tscores.append(\n",
    "\t\t\tmetrics.silhouette_score(X, km.labels_, sample_size=4000)\n",
    "\t\t)\n",
    "\ttrain_times = np.asarray(train_times)\n",
    "\n",
    "\tprint(f\"clustering done in {humanize.precisedelta(train_times.mean(), suppress=['days', 'microseconds'])} ± {humanize.precisedelta(train_times.std(), suppress=['days', 'microseconds'])} \")\n",
    "\tevaluation = {\n",
    "\t\t\"estimator\": name,\n",
    "\t\t\"train_time\": train_times.mean(),\n",
    "\t}\n",
    "\tevaluation_std = {\n",
    "\t\t\"estimator\": name,\n",
    "\t\t\"train_time\": train_times.std(),\n",
    "\t}\n",
    "\tmean_score, std_score = np.mean(scores), np.std(scores)\n",
    "\tprint(f\"Silhouette Coefficient: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "\t# evaluations.append(evaluation)\n",
    "\t# evaluations_std.append(evaluation_std)\n",
    "\n",
    "start_time = time.time()\n",
    "for seed in range(5):\n",
    "\tkmeans = KMeans(\n",
    "\t\tn_clusters=n_topics,\n",
    "\t\tmax_iter=100,\n",
    "\t\tn_init=1,\n",
    "\t\trandom_state=seed,\n",
    "\t).fit(TFIDFvectorised_dataset)\n",
    "\tcluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "\tprint(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "\n",
    "kmeans = KMeans(\n",
    "\tn_clusters=n_topics,\n",
    "\tmax_iter=100,\n",
    "\tn_init=10,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, TFIDFvectorised_dataset, name=\"KMeans on tf-idf vectors\")\n",
    "print(f\"kmeans clustering done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "\tfig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "\taxes = axes.flatten()\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\ttop_features_ind = topic.argsort()[-n_top_words:]\n",
    "\t\ttop_features = feature_names[top_features_ind]\n",
    "\t\tweights = topic[top_features_ind]\n",
    "\n",
    "\t\tax = axes[topic_idx]\n",
    "\t\tax.barh(top_features, weights, height=0.7)\n",
    "\t\tax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "\t\tax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\t\tfor i in \"top right left\".split():\n",
    "\t\t\tax.spines[i].set_visible(False)\n",
    "\t\tfig.suptitle(title, fontsize=40)\n",
    "\n",
    "\tplt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features\")\n",
    "start_time = time.time()\n",
    "FrobNMF = NMF(\n",
    "\tn_components=n_topics,\n",
    "\trandom_state=1,\n",
    "\tinit=init,\n",
    "\tbeta_loss=\"frobenius\",\n",
    "\tsolver=\"mu\",\n",
    "\talpha_W=0.00005,\n",
    "\talpha_H=0.00005,\n",
    "\tl1_ratio=1,\n",
    ").fit(TFIDFvectorised_dataset)\n",
    "print(f\"Done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "\n",
    "print(\"\\n\", \"Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features\")\n",
    "KLNMF = NMF(\n",
    "\tn_components=n_topics,\n",
    "\trandom_state=1,\n",
    "\tinit=init,\n",
    "\tbeta_loss=\"kullback-leibler\",\n",
    "\tsolver=\"mu\",\n",
    "\tmax_iter=1000,\n",
    "\talpha_W=0.00005,\n",
    "\talpha_H=0.00005,\n",
    "\tl1_ratio=0.5,\n",
    ").fit(TFIDFvectorised_dataset)\n",
    "print(f\"Done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "\n",
    "print(\"\\n\", \"Fitting the MiniBatchNMF model (Frobenius norm) with tf-idf\")\n",
    "FrobMBNMF = MiniBatchNMF(\n",
    "\tn_components=n_topics,\n",
    "\trandom_state=1,\n",
    "\tbatch_size=batch_size,\n",
    "\tinit=init,\n",
    "\tbeta_loss=\"frobenius\",\n",
    "\talpha_W=0.00005,\n",
    "\talpha_H=0.00005,\n",
    "\tl1_ratio=0.5,\n",
    ").fit(TFIDFvectorised_dataset)\n",
    "print(f\"Done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "\n",
    "print(\"\\n\", \"Fitting the MiniBatchNMF model (generalized Kullback-Leibler divergence) with tf-idf\")\n",
    "KLMBNMF = MiniBatchNMF(\n",
    "\tn_components=n_topics,\n",
    "\trandom_state=1,\n",
    "\tbatch_size=batch_size,\n",
    "\tinit=init,\n",
    "\tbeta_loss=\"kullback-leibler\",\n",
    "\talpha_W=0.00005,\n",
    "\talpha_H=0.00005,\n",
    "\tl1_ratio=0.5,\n",
    ").fit(TFIDFvectorised_dataset)\n",
    "print(f\"Done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "\n",
    "tfidf_feature_names = TFIDFvectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Plotting...\")\n",
    "n_top_words = 20\n",
    "plot_top_words(\n",
    "\tFrobNMF,\n",
    "\ttfidf_feature_names,\n",
    "\tn_top_words,\n",
    "\t\"Topics in NMF model (Frobenius norm)\",\n",
    ")\n",
    "\n",
    "plot_top_words(\n",
    "\tKLNMF,\n",
    "\ttfidf_feature_names,\n",
    "\tn_top_words,\n",
    "\t\"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
    ")\n",
    "\n",
    "plot_top_words(\n",
    "\tFrobMBNMF,\n",
    "\ttfidf_feature_names,\n",
    "\tn_top_words,\n",
    "\t\"Topics in MiniBatchNMF model (Frobenius norm)\",\n",
    ")\n",
    "\n",
    "plot_top_words(\n",
    "\tKLMBNMF,\n",
    "\ttfidf_feature_names,\n",
    "\tn_top_words,\n",
    "\t\"Topics in MiniBatchNMF model (generalized Kullback-Leibler divergence)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
