{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import humanize\n",
    "import psutil\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import networkx as nx\n",
    "import igraph\n",
    "import leidenalg\n",
    "\n",
    "import numpy as np\n",
    "from wisdom_of_crowds import Crowd\n",
    "from wisdom_of_crowds import make_sullivanplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, MiniBatchNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_STRING = \"mongodb://JamIs:morticiaetpollito@118.138.244.29:27017/\"\n",
    "\n",
    "class Tweet:\n",
    "\tdef __init__(self, tweet):\n",
    "\t\tself.id = tweet['id']\n",
    "\t\tself.user = tweet['user']\n",
    "\t\tself.connected_user = tweet['connected_user']\n",
    "\t\tself.connection_type = tweet['connection_type']\n",
    "\t\tself.text = tweet['text']\n",
    "\n",
    "query = {\"$and\":\n",
    "        \t[\n",
    "            \t{\"datetime\": {\"$eq\": None}},\n",
    "            \t{\"lang\": 'en'},\n",
    "            \t{\"connection_type\": {\"$exists\": \"true\"}},\n",
    "            \t{\"connection_type\": {\"$ne\": None}},\n",
    "            \t{\"connected_user\": {\"$ne\": None}}\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting...\n",
      "connected\n"
     ]
    }
   ],
   "source": [
    "print(\"connecting...\")\n",
    "client = MongoClient(CONNECTION_STRING)\n",
    "tw_coll = client.get_database('Tw_Covid_DB').get_collection('tweets')\n",
    "tu_coll = client.get_database('Tw_Covid_DB').get_collection('users')\n",
    "print(\"connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 182020\n"
     ]
    }
   ],
   "source": [
    "dates = {}\n",
    "\n",
    "for tweet in tw_coll.find():\n",
    "\t#check date of tweet\n",
    "\tmessy_date = tweet[\"created_at\"] #\"Thu Mar 12 02:01:57 +0000 2020\"\n",
    "\treal_date = messy_date[:10] + messy_date[-4:]\n",
    "\n",
    "\tif real_date in dates.keys():\n",
    "\t\t#increment counter for each date\n",
    "\t\tdates[real_date] += 1\n",
    "\telse:\n",
    "\t\tdates[real_date] = 1\n",
    "\n",
    "best_date = None\n",
    "for date in dates.keys():\n",
    "\tif best_date is None:\n",
    "\t\tbest_date = date\n",
    "\tif dates[best_date] < dates[date]:\n",
    "\t\tbest_date = date\n",
    "\n",
    "print(best_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7616588\n"
     ]
    }
   ],
   "source": [
    "query_results = tw_coll.find(query)\n",
    "tweets = []\n",
    "for t in query_results:\n",
    "\ttweets.append(Tweet(t))\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dict' object has no attribute 'user'\n",
      "Tweets loaded: 0\n",
      "Time taken: 0 seconds\n",
      "Memory used: 11.2 GB\n"
     ]
    }
   ],
   "source": [
    "tweets_by_id = {}\n",
    "final_tweets = {}\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "\tfor t in tweets:\n",
    "\t\t#we do a mapping of tweets to users this way so we can filter users based on ...location? eventually\n",
    "\t\t#user_tweet_mapping is an interim mapping we use to get the users from the database. At query time we add any additional criteria\n",
    "\t\t#tweets is a interm list of Tweets objects so we can keep the tweet id, users id, and connected_user id bundled during the batched query\n",
    "\t\ttweets_by_id[t.id] = t\n",
    "\t\tif t.user not in user_tweet_mapping:\n",
    "\t\t\tuser_tweet_mapping[t.user] = []\n",
    "\t\tif t.connected_user not in user_tweet_mapping:\n",
    "\t\t\tuser_tweet_mapping[t.connected_user] = []\n",
    "\t\tuser_tweet_mapping[t.user].append(t.id) \n",
    "\t\tuser_tweet_mapping[t.connected_user].append(t.id)\n",
    "\t\t\n",
    "\t\tif len(user_tweet_mapping) > 4096:\n",
    "\t\t\tuser_list = user_tweet_mapping.keys()\n",
    "\t\t\tusers = tu_coll.find({\"_id\":{\"$in\":list(user_list)}})#list()}}) #get the users\n",
    "\t\t\tfor user in users:\n",
    "\t\t\t\t# if the user is found by our constrained query, the Tweet object with the bundled tweet id, users id, and connected_user id\n",
    "\t\t\t\t# is added to the final_tweets list. The final_tweets list therefore only has tweets by users who meet our criteria\n",
    "\t\t\t\tfor tweet_id in user_tweet_mapping[user['id']]:\n",
    "\t\t\t\t\tfinal_tweets[tweet_id] = tweets[tweet_id]\n",
    "\t\t\tuser_tweet_mapping = {} #reset\n",
    "except Exception as e:\n",
    "\tprint(e)\n",
    "finally:\n",
    "\tend_time = time.time()\n",
    "\tprint(\"Tweets loaded: {}\".format(len(final_tweets)))\n",
    "\tprint(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "\tprint(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ig_plot(i_g, layout=\"auto\", filename=None):\n",
    "\tprint(f\"plotting {layout}:\")\n",
    "\tfilename = filename or f\"ig_{layout}.pdf\"\n",
    "\tstart_time = time.time()\n",
    "\tif hasattr(i_g, 'membership'):\n",
    "\t\tvertex_colour = i_g.membership\n",
    "\telse:\n",
    "\t\tvertex_colour = \"black\"\n",
    "\tigraph.plot(\n",
    "\t\ti_g, \n",
    "\t\tlayout=layout, \n",
    "\t\ttarget=filename, \n",
    "\t\tvertex_size=5, \n",
    "\t\tedge_arrow_size=0.5, \n",
    "\t\tedge_arrow_width = 0.5,\n",
    "\t\t#vertex_label = i_g.vs[\"name\"],\n",
    "\t\tvertex_color=vertex_colour,\n",
    "\t\tpalette=igraph.RainbowPalette(),\n",
    "\t)\n",
    "\tend_time = time.time()\n",
    "\tprint(\"plotting complete\")\n",
    "\tprint(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "\tprint(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building networkx graph...\n",
      "Time taken: 2 minutes and 3 seconds\n",
      "Memory used: 12.1 GB\n",
      "Nodes: 3617873\n",
      "Edges: 7220344\n",
      "Density: 5.516344421444012e-07\n",
      "networkx graph built\n"
     ]
    }
   ],
   "source": [
    "nx_g = nx.DiGraph()\n",
    "print(\"building networkx graph...\")\n",
    "start_time = time.time()\n",
    "for tweet in tweets:\n",
    "\tif tweet.user not in nx_g:\n",
    "\t\tnx_g.add_node(tweet.user)\n",
    "\t\n",
    "\tif tweet.connected_user not in nx_g:\n",
    "\t\tnx_g.add_node(tweet.connected_user)\n",
    "\t\n",
    "\tif tweet.connected_user != tweet.user:\n",
    "\t\tif tweet.connected_user not in nx_g[tweet.user]:\n",
    "\t\t\tnx_g.add_edge(tweet.user, tweet.connected_user, weight=1, tweets=[tweet.id])\n",
    "\t\telse:\n",
    "\t\t\tnx_g[tweet.user][tweet.connected_user]['weight'] += 1\n",
    "\t\t\tnx_g[tweet.user][tweet.connected_user]['tweets'] += tweet.id\n",
    "end_time = time.time()\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "print(\"Nodes: {}\".format(nx_g.number_of_nodes()))\n",
    "print(\"Edges: {}\".format(nx_g.number_of_edges()))\n",
    "print(\"Density: {}\".format(nx.density(nx_g)))\n",
    "print(\"networkx graph built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building igraph from networkx graph\n",
      "Time taken: 22 seconds\n",
      "Memory used: 12.1 GB\n",
      "Nodes: 3617873\n",
      "Edges: 7220344\n",
      "Transitivity: 0.00010257998573380519\n",
      "igraph from networkx built\n"
     ]
    }
   ],
   "source": [
    "print(\"building igraph from networkx graph\")\n",
    "start_time = time.time()\n",
    "i_g = igraph.Graph.from_networkx(nx_g, vertex_attr_hashable=\"name\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "print(\"Nodes: {}\".format(len(i_g.vs)))\n",
    "print(\"Edges: {}\".format(len(i_g.es)))\n",
    "print(\"Transitivity: {}\".format(i_g.transitivity_undirected()))\n",
    "print(\"igraph from networkx built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leidenalg:\n",
      "Graphs: 2021\n",
      "leidenalg analysis complete\n",
      "Time taken: 1 minute\n",
      "Memory used: 12.2 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"leidenalg:\")\n",
    "start_time = time.time()\n",
    "ig_community_graph = leidenalg.find_partition(i_g.connected_components(\"weak\").giant(), leidenalg.ModularityVertexPartition);\n",
    "print(\"Graphs: {}\".format(len(ig_community_graph.subgraphs())))\n",
    "end_time = time.time()\n",
    "print(\"leidenalg analysis complete\")\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_plot(ig_community_graph, \"fruchterman_reingold\", filename=\"leidencommunities.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraph 1: 670582 nodes\n",
      "Subgraph 1 as a proportion of total: 0.18535255383480848 nodes\n",
      "Subgraph 2: 378722 nodes\n",
      "Subgraph 2 as a proportion of total: 0.10468084424190678 nodes\n",
      "Subgraph 3: 224830 nodes\n",
      "Subgraph 3 as a proportion of total: 0.062144248844555904 nodes\n",
      "Subgraph 4: 218416 nodes\n",
      "Subgraph 4 as a proportion of total: 0.06037138395958067 nodes\n",
      "Small Graphs: 1604\n"
     ]
    }
   ],
   "source": [
    "# Get all subgraphs\n",
    "subgraphs = ig_community_graph.subgraphs()\n",
    "# Sort subgraphs by size in descending order\n",
    "sorted_subgraphs = sorted(subgraphs, key=lambda x: len(x.vs), reverse=True)\n",
    "# Get the largest 4 subgraphs\n",
    "largest_4_subgraphs = sorted_subgraphs[:4]\n",
    "for i, subgraph in enumerate(largest_4_subgraphs):\n",
    "\tprint(f\"Subgraph {i + 1}: {len(subgraph.vs)} nodes\")\n",
    "\tprint(f\"Subgraph {i + 1} as a proportion of total: {len(subgraph.vs)/len(i_g.vs)} nodes\")\n",
    "# Filter subgraphs with less than 10 nodes\n",
    "small_subgraphs = [sg for sg in subgraphs if len(sg.vs) < 10]\n",
    "print(\"Small Graphs: {}\".format(len(small_subgraphs)))\n",
    "# Count the number of small subgraphs\n",
    "num_small_subgraphs = len(small_subgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five edges in community:\n",
      "52482 -> 52483: 6 tweets\n",
      "4197 -> 2896: 5 tweets\n",
      "5389 -> 16816: 5 tweets\n",
      "82455 -> 16816: 4 tweets\n",
      "107104 -> 107105: 4 tweets\n",
      "Top five edges in community:\n",
      "203436 -> 707: 52 tweets\n",
      "92490 -> 171: 48 tweets\n",
      "11187 -> 11186: 44 tweets\n",
      "9806 -> 70: 43 tweets\n",
      "4060 -> 70: 42 tweets\n",
      "Top five edges in community:\n",
      "543 -> 15: 95 tweets\n",
      "32939 -> 7500: 62 tweets\n",
      "20537 -> 13657: 47 tweets\n",
      "130709 -> 13657: 44 tweets\n",
      "27953 -> 1890: 40 tweets\n",
      "Top five edges in community:\n",
      "3472 -> 3473: 72 tweets\n",
      "9254 -> 3473: 62 tweets\n",
      "10561 -> 10562: 52 tweets\n",
      "6088 -> 42: 28 tweets\n",
      "3473 -> 3472: 25 tweets\n"
     ]
    }
   ],
   "source": [
    "for community in largest_4_subgraphs:\n",
    "\t#order the edges in a community by weight\n",
    "\tedges = sorted(community.es, key=lambda edge: edge['weight'], reverse=True)\n",
    "\ttop_five = edges[:5]\n",
    "\n",
    "\t# Print the top five edges in the community\n",
    "\tprint(f\"Top five edges in community:\")\n",
    "\tfor edge in top_five:\n",
    "\t\tprint(f\"{edge.source} -> {edge.target}: {edge['weight']} tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transferring igraph communities to networkx\n",
      "0\n",
      "3617873\n",
      "Time taken: 57 seconds\n",
      "Memory used: 12.5 GB\n",
      "igraph communities transferred to networkx\n"
     ]
    }
   ],
   "source": [
    "print(\"transferring igraph communities to networkx\")\n",
    "start_time = time.time()\n",
    "#shift community info from igraph to networkx\n",
    "#for each node in networkx graph\n",
    "#look up matching node in igraph\n",
    "#assign T property of networkx graph node to community value from igraph node\n",
    "# Get the membership list from the igraph partition\n",
    "membership = ig_community_graph.membership\n",
    "# For each node in the networkx graph\n",
    "for node in nx_g.nodes():\n",
    "\t# Look up the matching node in the igraph graph\n",
    "\tig_node_index = i_g.vs.find('_nx_name'==node).index\n",
    "\t# Assign the 'T' property of the networkx node to the community value from the igraph node\n",
    "\tnx_g.nodes[node]['T'] = membership[ig_node_index]\n",
    "#delete all nodes that don't have a membership value - these nodes weren't in the igraph largest connected component\n",
    "print(len([node for node in nx_g if 'T' not in nx_g.nodes[node]]))\n",
    "print(len([node for node in nx_g if 'T' in nx_g.nodes[node]]))\n",
    "nx_g.remove_nodes_from([node for node in nx_g if 'T' not in nx_g.nodes[node]])\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "print(\"igraph communities transferred to networkx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do wisdom of the crowds analysis\n",
    "print(\"wisdom of the crowds:\")\n",
    "start_time = time.time()\n",
    "c = Crowd(nx_g)\n",
    "s_set = []\n",
    "d_set = []\n",
    "for node in c.node_set:\n",
    "\ts_set.append(c.S(node))\n",
    "\td_set.append(c.D(node))\n",
    "s_set = np.array(s_set)\n",
    "d_set = np.array(d_set)\n",
    "π_set = np.multiply(s_set,d_set)\n",
    "print(\"Time taken: {}\".format(humanize.precisedelta(end_time - start_time, suppress=['days', 'milliseconds', 'microseconds'])))\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "print(\"wisdom of the crowds complete\")\n",
    "make_sullivanplot(π_set,d_set,s_set)\n",
    "make_sullivanplot(π_set,d_set,s_set,colormap='magma_r',yscale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "init = \"nndsvda\"\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorising (TF-IDF)...\n",
      "n_samples/documents: 7616588, n_features/words: 9\n",
      "Sparsity (number of cells with non-zero values): 0.219\n",
      "vectorization done in 1 minute and 44 seconds\n",
      "Memory used: 12.8 GB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Add your custom words\n",
    "# Note: “WuhanVirus” not included as it is politically charged\n",
    "custom_words = [\"coronavirus\", \"2019nCoV\", \"corona virus\", \"COVD19\", \"CoronavirusPandemic\", \"COVID-19\", \"CoronaOutbreak\", \n",
    "\t\t\t\t\"pneumonia\", \"pneumonie\", \"neumonia\", \"lungenentzündung\", \"COVID19\", \"http\", \"https\", \"https://\", \"19\", \"just\", \"people\", \"rt\"]\n",
    "\n",
    "# Extend the default English stop words list with your words\n",
    "stop_words = ENGLISH_STOP_WORDS.union(custom_words)\n",
    "\n",
    "print(\"Vectorising (TF-IDF)...\")\n",
    "TFIDFvectorizer = TfidfVectorizer(\n",
    "\tmin_df=0.05,\n",
    "\tstop_words=stop_words\n",
    ")\n",
    "start_time = time.time()\n",
    "TFIDFvectorised_dataset = TFIDFvectorizer.fit_transform([tweet.text for tweet in tweets])\n",
    "print(f\"n_samples/documents: {TFIDFvectorised_dataset.shape[0]}, n_features/words: {TFIDFvectorised_dataset.shape[1]}\") #shape is rows of the matrix in that dimension\n",
    "print(f\"Sparsity (number of cells with non-zero values): {TFIDFvectorised_dataset.nnz / np.prod(TFIDFvectorised_dataset.shape):.3f}\")\n",
    "print(f\"vectorization done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering...\n",
      "Number of elements assigned to each cluster: [ 624549 1629895 1929417  490726  699247  812991  497375  446619  148491\n",
      "  337278]\n",
      "Number of elements assigned to each cluster: [ 105428 1929417  342789  476218 1308763  624745  497380  435971  366553\n",
      " 1529324]\n",
      "Number of elements assigned to each cluster: [1629895 1929417  699247  490726  497375  446619  624549  337278  812991\n",
      "  148491]\n",
      "Number of elements assigned to each cluster: [ 435774 1629921 1929417 1304531  342789  497375  624549  366267  337278\n",
      "  148687]\n",
      "Number of elements assigned to each cluster: [1313588 1929417 1629921   54266  342789  476214  497380  365438  435972\n",
      "  571603]\n"
     ]
    }
   ],
   "source": [
    "print(\"Kmeans clustering...\")\n",
    "\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
    "\tname = km.__class__.__name__ if name is None else name\n",
    "\n",
    "\ttrain_times = []\n",
    "\tscores = [] #score = \"Silhouette Coefficient\"\n",
    "\tfor seed in range(n_runs):\n",
    "\t\tkm.set_params(random_state=seed)\n",
    "\t\tstart_time = time.time()\n",
    "\t\tkm.fit(X)\n",
    "\t\ttrain_times.append(time.time() - start_time)\n",
    "\t\tscores.append(\n",
    "\t\t\tmetrics.silhouette_score(X, km.labels_, sample_size=4000)\n",
    "\t\t)\n",
    "\ttrain_times = np.asarray(train_times)\n",
    "\n",
    "\tprint(f\"clustering done in {humanize.precisedelta(train_times.mean(), suppress=['days', 'microseconds'])} ± {humanize.precisedelta(train_times.std(), suppress=['days', 'microseconds'])} \")\n",
    "\tevaluation = {\n",
    "\t\t\"estimator\": name,\n",
    "\t\t\"train_time\": train_times.mean(),\n",
    "\t}\n",
    "\tevaluation_std = {\n",
    "\t\t\"estimator\": name,\n",
    "\t\t\"train_time\": train_times.std(),\n",
    "\t}\n",
    "\tmean_score, std_score = np.mean(scores), np.std(scores)\n",
    "\tprint(f\"Silhouette Coefficient: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "\t# evaluations.append(evaluation)\n",
    "\t# evaluations_std.append(evaluation_std)\n",
    "\n",
    "start_time = time.time()\n",
    "for seed in range(5):\n",
    "\tkmeans = KMeans(\n",
    "\t\tn_clusters=n_topics,\n",
    "\t\tmax_iter=100,\n",
    "\t\tn_init=1,\n",
    "\t\trandom_state=seed,\n",
    "\t).fit(TFIDFvectorised_dataset)\n",
    "\tcluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "\tprint(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "\n",
    "kmeans = KMeans(\n",
    "\tn_clusters=n_topics,\n",
    "\tmax_iter=100,\n",
    "\tn_init=10,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, TFIDFvectorised_dataset, name=\"KMeans on tf-idf vectors\")\n",
    "print(f\"kmeans clustering done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "\tfig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "\taxes = axes.flatten()\n",
    "\tfor topic_idx, topic in enumerate(model.components_):\n",
    "\t\ttop_features_ind = topic.argsort()[-n_top_words:]\n",
    "\t\ttop_features = feature_names[top_features_ind]\n",
    "\t\tweights = topic[top_features_ind]\n",
    "\n",
    "\t\tax = axes[topic_idx]\n",
    "\t\tax.barh(top_features, weights, height=0.7)\n",
    "\t\tax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "\t\tax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\t\tfor i in \"top right left\".split():\n",
    "\t\t\tax.spines[i].set_visible(False)\n",
    "\t\tfig.suptitle(title, fontsize=40)\n",
    "\n",
    "\tplt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features\")\n",
    "start_time = time.time()\n",
    "FrobNMF = NMF(\n",
    "\tn_components=n_topics,\n",
    "\trandom_state=1,\n",
    "\tinit=init,\n",
    "\tbeta_loss=\"frobenius\",\n",
    "\tsolver=\"mu\",\n",
    "\talpha_W=0.00005,\n",
    "\talpha_H=0.00005,\n",
    "\tl1_ratio=1,\n",
    ").fit(TFIDFvectorised_dataset)\n",
    "print(f\"Done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "\n",
    "print(\"\\n\", \"Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features\")\n",
    "KLNMF = NMF(\n",
    "\tn_components=n_topics,\n",
    "\trandom_state=1,\n",
    "\tinit=init,\n",
    "\tbeta_loss=\"kullback-leibler\",\n",
    "\tsolver=\"mu\",\n",
    "\tmax_iter=1000,\n",
    "\talpha_W=0.00005,\n",
    "\talpha_H=0.00005,\n",
    "\tl1_ratio=0.5,\n",
    ").fit(TFIDFvectorised_dataset)\n",
    "print(f\"Done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "\n",
    "print(\"\\n\", \"Fitting the MiniBatchNMF model (Frobenius norm) with tf-idf\")\n",
    "FrobMBNMF = MiniBatchNMF(\n",
    "\tn_components=n_topics,\n",
    "\trandom_state=1,\n",
    "\tbatch_size=batch_size,\n",
    "\tinit=init,\n",
    "\tbeta_loss=\"frobenius\",\n",
    "\talpha_W=0.00005,\n",
    "\talpha_H=0.00005,\n",
    "\tl1_ratio=0.5,\n",
    ").fit(TFIDFvectorised_dataset)\n",
    "print(f\"Done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "\n",
    "print(\"\\n\", \"Fitting the MiniBatchNMF model (generalized Kullback-Leibler divergence) with tf-idf\")\n",
    "KLMBNMF = MiniBatchNMF(\n",
    "\tn_components=n_topics,\n",
    "\trandom_state=1,\n",
    "\tbatch_size=batch_size,\n",
    "\tinit=init,\n",
    "\tbeta_loss=\"kullback-leibler\",\n",
    "\talpha_W=0.00005,\n",
    "\talpha_H=0.00005,\n",
    "\tl1_ratio=0.5,\n",
    ").fit(TFIDFvectorised_dataset)\n",
    "print(f\"Done in {humanize.precisedelta(time.time() - start_time, suppress=['days', 'microseconds'])}\")\n",
    "print(\"Memory used: {}\".format(humanize.naturalsize(psutil.Process().memory_info().rss)))\n",
    "\n",
    "tfidf_feature_names = TFIDFvectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Plotting...\")\n",
    "n_top_words = 20\n",
    "plot_top_words(\n",
    "\tFrobNMF,\n",
    "\ttfidf_feature_names,\n",
    "\tn_top_words,\n",
    "\t\"Topics in NMF model (Frobenius norm)\",\n",
    ")\n",
    "\n",
    "plot_top_words(\n",
    "\tKLNMF,\n",
    "\ttfidf_feature_names,\n",
    "\tn_top_words,\n",
    "\t\"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
    ")\n",
    "\n",
    "plot_top_words(\n",
    "\tFrobMBNMF,\n",
    "\ttfidf_feature_names,\n",
    "\tn_top_words,\n",
    "\t\"Topics in MiniBatchNMF model (Frobenius norm)\",\n",
    ")\n",
    "\n",
    "plot_top_words(\n",
    "\tKLMBNMF,\n",
    "\ttfidf_feature_names,\n",
    "\tn_top_words,\n",
    "\t\"Topics in MiniBatchNMF model (generalized Kullback-Leibler divergence)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
